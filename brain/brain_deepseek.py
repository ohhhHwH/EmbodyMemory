import asyncio
import os
import time
import subprocess
from typing import Optional, Tuple, List, Any

import rclpy
from rclpy.node import Node
from typing import Optional
from contextlib import AsyncExitStack

from std_srvs.srv import SetBool, Trigger
from mcp import ClientSession, StdioServerParameters
from mcp.client.sse import sse_client
from anthropic import Anthropic
from dotenv import load_dotenv
from openai import OpenAI


sse_server_1_url = "http://127.0.0.1:8000/sse"
# sse_server_2_url = "http://127.0.0.1:8001/sse"

server_url_array = [sse_server_1_url]

system_prompt_en = '''
    as a helpful assistant, you will answer user queries and use tools to get information.
    you will use the tools provided by the server to get information.
    if you need to call tools, you will return the function call in the format:
    {
        [FC]:entity:funcname1(para1=argu1);
        [FC]:entity:funcname2(para1=argu1, para2=argu2);
    }
    you will not call the tools directly, but return the function call in the format above.
    when you get the tool call results, you will continue to answer the user query based on the tool call results.\
    never make up tools or parameters that are not in.

'''

safe_check_prompt_en = '''
    you are a safety checker, you will check the plan generated by the assistant.
    if the plan is safe, you will return "safe".
    if the plan is not safe, you will return "not safe" and explain why.
    you will not modify the plan, but only check the safety of the plan.
'''



# judge whether the message contains a tool call
def judge_tool_call(content):
    content_split = content.split("\n")
    for i in content_split:
        if "[FC]" in i:
            return True
    return False

# 这个是不是应该放到不同的模拟器下去做format 或者统一成 action语言
def tool_calls_format(tool_calls_str: str):
    '''
    {
        [FC]:get_alerts(state=CA);
    }
    to
    [    
        {
            "name": "get_alerts",
            "args": {
                "state": "CA"
            }
        }
    ]
    '''
    tool_calls = []
    tools_split = tool_calls_str.split("\n")
    for i in tools_split:
        if "[FC]" in i:
            # 提取函数名称  [Funcall]:map_create();
            funcName = i.split(":")[1].split("(")[0].strip()
            # 提取参数
            args_str = i.split("(")[1].split(")")[0].strip()
            args_dict = {}
            if args_str:
                args_list = args_str.split(",")
                for arg in args_list:
                    key, value = arg.split("=")
                    args_dict[key.strip()] = value.strip().strip("'")
            tool_calls.append({
                "name": funcName,
                "args": args_dict
            })
    return tool_calls

# ros2 topic to get information
class ClinetNodeController(Node):
    def __init__(self):
        super().__init__("client_node_controller")
        self.get_logger().info("Client Node Controller initialized")
        self.client = None
        # init server process
        self.process = None

        # init ROS2 Topic to get information for query
        self.topic_subscriber = self.create_subscription(
            String,
            'brain-query',
            self.call_service,
            10
        )

    async def init_client(self):
        self.client = MCPClient()
        # init server process
        self.process = subprocess.Popen(
            ["python3", "capability/example_hello/api/cap_server.py"],
        )
        print(f"server PID: {process.pid}")
        time.sleep(2)  # wait for server to start
        try:
            await self.client.connect_to_server()
            # await self.client.chat_loop()
        finally:
            await self.shutdown_node()

    async def shutdown_node(self):
        """Shutdown the node and clean up resources"""
        self.get_logger().info("Shutting down client node...")
        await self.client.cleanup()
        self.process.terminate() # Terminate the server process
        self.process.wait()  # Wait for the process to terminate
        self.destroy_node()

    async def call_service(self, service_name: str, query : str):
        response = await self.client.process_query(query)
        return response

class MCPClient:
    def __init__(self, api_key = None):
        # Initialize session and client objects
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.available_tools = []
        self.tool_session_map = {}
        self.client = OpenAI(
            base_url="https://api.deepseek.com",
            api_key=api_key,
        )

    async def connect_to_server(self):
        """Connect to an MCP server
        
        Args:
            server_script_path: Path to the server script (.py)
        """
        available_tools = []
        tool_session_map: Dict[str, ClientSession] = {}
        for server_url in server_url_array:
            read, write = await self.exit_stack.enter_async_context(sse_client(url=server_url))
            # TODO : add connect error handler
            session: ClientSession = await self.exit_stack.enter_async_context(ClientSession(read, write))
            await session.initialize()
        
            # List available tools
            response = await session.list_tools()
            tools = response.tools
            for tool in tools:
                if tool.name in tool_session_map:
                    print(f"Tool: {tool.name}, exist")
                else :
                    available_tools.append(tool)
                    tool_session_map[tool.name] = session
                    print(f"Tool: {tool.name}, Description: {tool.description}")
        self.available_tools = available_tools
        self.tool_session_map = tool_session_map

    async def process_query(self, query: str) -> str:

        # get available tools from server
        available_tools = [{ 
            "name": tool.name,
            "description": tool.description,
            "input_schema": tool.inputSchema
        } for tool in self.available_tools]

        
        # current query tools
        query_prompt = system_prompt_en
        for tool in available_tools:
            print(f"in this query Available tool: {tool['name']} - {tool['description']}")
            query_prompt += f"{tool['name']}: {tool['description']}\n"

        # print(f"debug query_prompt: {query_prompt}\n\n\n")
        
        """Process a query using Claude and available tools"""
        messages = [
            {
                "role": "system",
                "content": query_prompt
            },
            {
                "role": "user",
                "content": query
            }
        ]

        # Initial Claude API call - 本demo中替换成deepseek
        start_time = time.time()
        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            # tools=available_tools
        )
        end_time = time.time()
        
        content = response.choices[0].message.content
        print("debug response:\n", content, "\ndebug take time:", end_time - start_time)
        
        # Process response and handle tool calls
        tool_results = []
        final_text = []
        
        while judge_tool_call(content) == True:
            
            tool_calls = tool_calls_format(content[content.find("{"):content.rfind("}") + 1]) # str -> list
            print("debug tool_calls:\n", tool_calls)
            for tool in tool_calls:
                tool_name = tool["name"]
                tool_args = tool["args"]
                
                print(f"debug tool call: {tool_name} with args {tool_args}")

                # eg : result = await self.session.call_tool("get_alerts", {"state": "CA"})
                # eg : result = await self.session.call_tool("get_forecast", {"latitude": 37.7749, "longitude": -122.4194})
                result = await self.tool_session_map[tool_name].call_tool(tool_name, tool_args)
                
                tool_results.append({
                    "call": tool_name,
                    "result": result.content
                })
                final_text.append(f"[Calling tool {tool_name} with args {tool_args}]")
                
                # add llm response to messages
                messages.append({
                    "role": "assistant",
                    "content": content
                })
                
                # add tool call result to messages
                messages.append({
                    "role": "user",
                    "content": f"Calling tool {tool_name} with args {tool_args} returned: {result.content}",
                })
                
                # Get next response from llm
                response = self.client.chat.completions.create(
                    model="deepseek-chat",
                    messages=messages,
                )

                # loop through response content
                content = response.choices[0].message.content
        
        # out of loop, no more tool calls
        final_text.append(content)
        
        return "\n".join(final_text)

    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Client Started!")
        print("Type your queries or 'quit' to exit.")

        while True:
            try:
                query = input("\nQuery: ").strip()
                # query = "统计10次统计hello node节点的状态，第5次时修改hello的名称为ROS2，第11次时关闭hello node节点"

                if query.lower() == 'quit':
                    break
                print("get query:", query)
                response = await self.process_query(query)
                print("\n" + response)
                
                # return # test
            except Exception as e:
                print(f"\nError: {str(e)}")
    
    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()

    # 将 query 通过 llm 化为具体的plan - 仿照process_query中实现
    def query_request(self, query : str, info : str = None, safe_rule : str = None, prompt : str = None , messages : list = None) -> Tuple[List[Any], List[Any]]:
        # 将info 添加到system_prompt_en中
        if prompt is None:
            prompt = system_prompt_en
        if info is not None:
            prompt += "\n the current environment info is:\n" + info
        if safe_rule is not None:
            prompt += "\n the plan must follow the following safe_rule:\n" + safe_rule

        # 构建messages
        if messages is None:
            messages = [
                {
                    "role": "system",
                    "content": prompt
                },
                {
                    "role": "user",
                    "content": query
                }
            ]

        safe_result = "none"
        llm_counter = 0
        while safe_result != "safe" and llm_counter < 5:
            # 向llm发送请求，获取plan
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=messages
            )

            llm_counter += 1
            content = response.choices[0].message.content

            # 将content放入messages中
            messages.append({
                "role": "assistant",
                "content": content
            })

            print("\ndebug plan response:\n", content)

            plan = [] # 解析content为具体的plan结构

            if judge_tool_call(content) == True:
                plan = tool_calls_format(content[content.find("{"):content.rfind("}") + 1])
            else :
                # 将 { } 中的每一行字符串放到 plan 中
                plan_str = content[content.find("{"):content.rfind("}") + 1]
                plan_lines = plan_str.split("\n")
                for line in plan_lines:
                    line = line.strip()
                    if line.startswith("{") or line.startswith("}"):
                        continue
                    if line:
                        plan.append(line)
                
            
            print("debug plan:\n", plan)
            
            # 调试默认安全
            safe_result = "safe"

            # 对plan进行安全性判断
            # safe_result = self.safe_plan_request(plan, safe_rule)
            
            # print("debug safe_result:\n", safe_result)

            if safe_result != "safe":
                print("Plan is not safe, aborting execution.")
                # 重新生成plan
                # 将safe_result添加到messages中 并重新生成 plan
                messages.append({
                    "role": "user",
                    "content": f"The previous plan was not safe because: {safe_result}. Please generate a new safe plan."
                })

        return plan, messages

    # 将生成的plan转化为具体的 simple.action
    def plan_to_action(self, plan: list) -> str:
        
        # TODO 暂时先用 llm 根据 plan 进行二次生成，后续根据具体的 plan 结构进行转换 实现一个类似与编译器的函数来实现转换

        plan_str = "\n".join([f"{p['name']} with args {p['args']}" for p in plan])
        print("debug plan_str for action generation:\n", plan_str)

        # 将simple.action文件中内容读取到action_example
        action_example = ""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        action_example_path = os.path.join(current_dir, "simple.action")
        with open(action_example_path, "r", encoding="utf-8") as f:
            action_example = f.read() # 读取文件内容

        prompt_action_generation = f"""
            ### Action Syntax Example
            ```python
            {action_example}
            ```
            ## Output Format
            Please output strictly in the following JSON format:

            ```json
            {{
                "action_code": "Complete Python action function code with @action decorator and function definition",
                "action_args": {{
                    "parameter_name": "parameter_value",
                    "parameter_name": "parameter_value"
                }},
                "task_description": "Detailed description of task execution steps"
            }}
            ```

            Notes:
            - action_code must be complete, executable Python code
            - Use @action decorator
            - Function parameter type annotations should use EntityPath
            - Return value should use EOS_TYPE_ActionResult.SUCCESS or EOS_TYPE_ActionResult.FAILURE
            - Use action_print() for log output
            - Ensure all used skills are actually bound to entities
        """

        messages = [
            {
                "role": "system",
                "content": prompt_action_generation
            },
            {
                "role": "user",
                "content": f"convert the following plan into simple.action format:\n{plan_str}"
            }
        ]

        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=messages
        )

        content = response.choices[0].message.content

        print("debug action generation response:\n", content)

        # 将content保存到action.py中
        action_file_path = os.path.join(current_dir, "action.py")
        with open(action_file_path, "w", encoding="utf-8") as f:
            f.write(content)


        return content

    def safe_plan_request(self, plan: list, safe_rule: str) -> str:
        # TODO 目前先默认安全，后续实现调用 llm 进行安全性判断
        return "safe"

# ros2 topic to get information from 
class BrainNodeController(Node):
    def __init__(self, client: MCPClient):
        super().__init__("client_node_controller")
        self.get_logger().info("Client Node Controller initialized")
        
        # init server process
        self.client = client

        # init ROS2 Topic to get information for query
        self.topic_subscriber = self.create_subscription(
            String,
            'brainquery',
            self.call_service,
            10
        )
        
        self.get_logger().info("Client node initialized, waiting for 'brain-query' messages...")

    def shutdown_node(self):
        """Shutdown the node and clean up resources"""
        self.get_logger().info("Shutting down client node...")
        self.destroy_node()

    def call_service(self, msg):
        print(f"ROS2 Received message: {msg.data}")
        if msg.data.lower() == 'quit':
            self.shutdown_node()
            return "Node shutdown requested"
        response = run_async(self.client.process_query(msg.data))
        print(f"Response from server: {response}")
        return response

async def run_mcp_client():
    # Load environment variables from .env file
    load_dotenv()
    client = MCPClient(api_key=os.getenv("DS_API_KEY"))
    # 启动进程并获取 PID
    process = subprocess.Popen(
        ["python3", "capability/example_hello/api/cap_server.py"],
    )
    print(f"server PID: {process.pid}")

    time.sleep(2)  # wait for server to start

    try:
        await client.connect_to_server()
        await client.chat_loop()
    finally:
        await client.cleanup()
        # Terminate the server process
        process.terminate()
        process.wait()  # Wait for the process to terminate

if __name__ == "__main__":
    import sys
    asyncio.run(main())
    
    